{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e1a4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import   tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import os # Added os for path joining\n",
    "\n",
    "# ==================================================================\n",
    "#                       1. DEFINE PATHS AND PARAMETERS\n",
    "# ==================================================================\n",
    "MODEL_PATH = r'plant_disease_model.h5'\n",
    "BASE_DIR = r'D:\\Projects\\Agri Detect\\PlantVillage-Dataset-master\\plant_dataset_split'\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "VALIDATION_DIR = os.path.join(BASE_DIR, 'validation')\n",
    "\n",
    "IMG_SIZE = (224, 224) # Use the same size you trained with\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCHS = 10  # The number of epochs you used for the first training\n",
    "\n",
    "# ==================================================================\n",
    "#                       2. SET UP DATA GENERATORS\n",
    "# ==================================================================\n",
    "\n",
    "# Add augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# NO augmentation for validation, just rescale\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# ==================================================================\n",
    "#                       3. LOAD AND UNFREEZE MODEL\n",
    "# ==================================================================\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# We will fine-tune from 'block_13_expand' onwards.\n",
    "# This means we will freeze all layers before it.\n",
    "fine_tune_at_layer = 'block_13_expand'\n",
    "\n",
    "# Unfreeze all layers first\n",
    "model.trainable = True\n",
    "\n",
    "# Find the index of the layer to fine-tune from\n",
    "try:\n",
    "    fine_tune_at_index = [i for i, layer in enumerate(model.layers) if layer.name == fine_tune_at_layer][0]\n",
    "except IndexError:\n",
    "    print(f\"Error: Layer '{fine_tune_at_layer}' not found in model.\")\n",
    "    print(\"Please check the name against the summary.\")\n",
    "    raise\n",
    "\n",
    "# Freeze all layers *before* that index\n",
    "print(f\"Freezing all layers up to layer #{fine_tune_at_index} ('{fine_tune_at_layer}')...\")\n",
    "for layer in model.layers[:fine_tune_at_index]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Model unfreezing complete.\")\n",
    "\n",
    "# ==================================================================\n",
    "#                       4. COMPILE WITH LOW LEARNING RATE\n",
    "# ==================================================================\n",
    "# This is the most important part of fine-tuning.\n",
    "# We use a *very* low learning rate so we don't destroy\n",
    "# the pre-trained weights.\n",
    "\n",
    "print(\"Re-compiling model with a low learning rate...\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # 0.00001\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the new trainable status\n",
    "model.summary()\n",
    "\n",
    "# ==================================================================\n",
    "#                       5. CONTINUE TRAINING (FINE-TUNE)\n",
    "# ==================================================================\n",
    "# We will train for 10 more epochs\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = INITIAL_EPOCHS + fine_tune_epochs\n",
    "\n",
    "# Add a callback to reduce the learning rate if it plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                 factor=0.2,\n",
    "                                 patience=2,\n",
    "                                 min_lr=1e-7)\n",
    "\n",
    "print(f\"Starting fine-tuning for {fine_tune_epochs} epochs...\")\n",
    "history_fine_tune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=INITIAL_EPOCHS, # This tells Keras to start at epoch 10\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# ==================================================================\n",
    "#                       6. SAVE YOUR NEW MODEL\n",
    "# ==================================================================\n",
    "NEW_MODEL_PATH = r'plant_disease_model_finetuned.h5'\n",
    "model.save(NEW_MODEL_PATH)\n",
    "print(f\"Fine-tuning complete! New model saved to: {NEW_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
